//#define SIMPLIFY

#include<iostream>
#include<algorithm>
#include<fstream>
#include<chrono>
#include<string>
#include<math.h>

#include<opencv2/core/core.hpp>

#include<System.h>

#include <stdio.h>
#ifdef _WIN32
#  define snprintf _snprintf
#endif
#include <stdlib.h>					// malloc(), free()
#ifdef __APPLE__
#  include <GLUT/glut.h>
#else
#  include <GL/glut.h>
#endif
#include <AR/config.h>
#include <AR/video.h>
#include <AR/param.h>			// arParamDisp()
#include <AR/ar.h>
#include <AR/gsub_lite.h>

using namespace std;

// ============================================================================
//	Constants
// ============================================================================

#define VIEW_SCALEFACTOR		1.0         // Units received from ARToolKit tracking will be multiplied by this factor before being used in OpenGL drawing.
#define VIEW_DISTANCE_MIN		40.0        // Objects closer to the camera than this will not be displayed. OpenGL units.
#define VIEW_DISTANCE_MAX		10000.0     // Objects further away from the camera than this will not be displayed. OpenGL units.


// ============================================================================
//	Global variables
// ============================================================================

// Preferences.
static int windowed = TRUE;                     // Use windowed (TRUE) or fullscreen mode (FALSE) on launch.
static int windowWidth = 640;					// Initial window width, also updated during program execution.
static int windowHeight = 480;                  // Initial window height, also updated during program execution.
static int windowDepth = 32;					// Fullscreen mode bit depth.
static int windowRefresh = 0;					// Fullscreen mode refresh rate. Set to 0 to use default rate.

// Image acquisition.
static ARUint8		*gARTImage = NULL;
static int          gARTImageSavePlease = FALSE;

// Marker detection.
static ARHandle		*gARHandle = NULL;
static ARPattHandle	*gARPattHandle = NULL;
static long			gCallCountMarkerDetect = 0;

// Transformation matrix retrieval.
static AR3DHandle	*gAR3DHandle = NULL;
static ARdouble		gPatt_width     = 80.0;	// Per-marker, but we are using only 1 marker.
static ARdouble		gPatt_trans[3][4];		// Per-marker, but we are using only 1 marker.
static int			gPatt_found = FALSE;	// Per-marker, but we are using only 1 marker.
static int			gPatt_id;				// Per-marker, but we are using only 1 marker.

// Drawing.
static ARParamLT *gCparamLT = NULL;
static ARGL_CONTEXT_SETTINGS_REF gArglSettings = NULL;
static int gShowHelp = 1;
static int gShowMode = 1;
static int gDrawRotate = FALSE;
static float gDrawRotateAngle = 0;			// For use in drawing.

void LoadImages(const string &strFile, vector<string> &vstrImageFilenames,
                vector<double> &vTimestamps);

string type2str(int type) {
    string r;

    uchar depth = type & CV_MAT_DEPTH_MASK;
    uchar chans = 1 + (type >> CV_CN_SHIFT);

    switch ( depth ) {
        case CV_8U:  r = "8U"; break;
        case CV_8S:  r = "8S"; break;
        case CV_16U: r = "16U"; break;
        case CV_16S: r = "16S"; break;
        case CV_32S: r = "32S"; break;
        case CV_32F: r = "32F"; break;
        case CV_64F: r = "64F"; break;
        default:     r = "User"; break;
    }

    r += "C";
    r += (chans+'0');

    return r;
}

static int setupMarker(const char *patt_name, int *patt_id, ARHandle *arhandle, ARPattHandle **pattHandle_p)
{
    if ((*pattHandle_p = arPattCreateHandle()) == NULL) {
        ARLOGe("setupMarker(): Error: arPattCreateHandle.\n");
        return (FALSE);
    }

	// Loading only 1 pattern in this example.
	if ((*patt_id = arPattLoad(*pattHandle_p, patt_name)) < 0) {
		ARLOGe("setupMarker(): Error loading pattern file %s.\n", patt_name);
		arPattDeleteHandle(*pattHandle_p);
		return (FALSE);
	}

    arPattAttach(arhandle, *pattHandle_p);

	return (TRUE);
}

static void DrawCube(void)
{
    // Colour cube data.
    int i;
	float fSize = 40.0f;
    const GLfloat cube_vertices [8][3] = {
        /* +z */ {0.5f, 0.5f, 0.5f}, {0.5f, -0.5f, 0.5f}, {-0.5f, -0.5f, 0.5f}, {-0.5f, 0.5f, 0.5f},
        /* -z */ {0.5f, 0.5f, -0.5f}, {0.5f, -0.5f, -0.5f}, {-0.5f, -0.5f, -0.5f}, {-0.5f, 0.5f, -0.5f} };
    const GLubyte cube_vertex_colors [8][4] = {
        {255, 255, 255, 255}, {255, 255, 0, 255}, {0, 255, 0, 255}, {0, 255, 255, 255},
        {255, 0, 255, 255}, {255, 0, 0, 255}, {0, 0, 0, 255}, {0, 0, 255, 255} };
    const GLubyte cube_faces [6][4] = { /* ccw-winding */
        /* +z */ {3, 2, 1, 0}, /* -y */ {2, 3, 7, 6}, /* +y */ {0, 1, 5, 4},
        /* -x */ {3, 0, 4, 7}, /* +x */ {1, 2, 6, 5}, /* -z */ {4, 5, 6, 7} };

    glPushMatrix(); // Save world coordinate system.
    glRotatef(gDrawRotateAngle, 0.0f, 0.0f, 1.0f); // Rotate about z axis.
    glScalef(fSize, fSize, fSize);
    glTranslatef(0.0f, 0.0f, 0.5f); // Place base of cube on marker surface.
    glDisable(GL_LIGHTING);
    glDisable(GL_TEXTURE_2D);
    glDisable(GL_BLEND);
    glColorPointer(4, GL_UNSIGNED_BYTE, 0, cube_vertex_colors);
    glVertexPointer(3, GL_FLOAT, 0, cube_vertices);
    glEnableClientState(GL_VERTEX_ARRAY);
    glEnableClientState(GL_COLOR_ARRAY);
    for (i = 0; i < 6; i++) {
        glDrawElements(GL_TRIANGLE_FAN, 4, GL_UNSIGNED_BYTE, &(cube_faces[i][0]));
    }
    glDisableClientState(GL_COLOR_ARRAY);
    glColor4ub(0, 0, 0, 255);
    for (i = 0; i < 6; i++) {
        glDrawElements(GL_LINE_LOOP, 4, GL_UNSIGNED_BYTE, &(cube_faces[i][0]));
    }
    glPopMatrix();    // Restore world coordinate system.
}

static void DrawCubeUpdate(float timeDelta)
{
	if (gDrawRotate) {
		gDrawRotateAngle += timeDelta * 45.0f; // Rotate cube at 45 degrees per second.
		if (gDrawRotateAngle > 360.0f) gDrawRotateAngle -= 360.0f;
	}
}

static void mainLoop(void)
{
    static int imageNumber = 0;
	static int ms_prev;
	int ms;
	float s_elapsed;
	ARUint8 *image;
	ARdouble err;

    int             j, k;

	// Find out how long since mainLoop() last ran.
	ms = glutGet(GLUT_ELAPSED_TIME);
	s_elapsed = (float)(ms - ms_prev) * 0.001f;
	if (s_elapsed < 0.01f) return; // Don't update more often than 100 Hz.
	ms_prev = ms;

	// Update drawing.
	DrawCubeUpdate(s_elapsed);

	// Grab a video frame.
	if ((image = arVideoGetImage()) != NULL) {
		gARTImage = image;	// Save the fetched image.

        if (gARTImageSavePlease) {
            char imageNumberText[15];
            sprintf(imageNumberText, "image-%04d.jpg", imageNumber++);
            if (arVideoSaveImageJPEG(gARHandle->xsize, gARHandle->ysize, gARHandle->arPixelFormat, gARTImage, imageNumberText, 75, 0) < 0) {
                ARLOGe("Error saving video image.\n");
            }
            gARTImageSavePlease = FALSE;
        }

		gCallCountMarkerDetect++; // Increment ARToolKit FPS counter.

		// Detect the markers in the video frame.
		if (arDetectMarker(gARHandle, gARTImage) < 0) {
			exit(-1);
		}

		// Check through the marker_info array for highest confidence
		// visible marker matching our preferred pattern.
		k = -1;
		for (j = 0; j < gARHandle->marker_num; j++) {
			if (gARHandle->markerInfo[j].id == gPatt_id) {
				if (k == -1) k = j; // First marker detected.
				else if (gARHandle->markerInfo[j].cf > gARHandle->markerInfo[k].cf) k = j; // Higher confidence marker detected.
			}
		}

		if (k != -1) {
			// Get the transformation between the marker and the real camera into gPatt_trans.
			err = arGetTransMatSquare(gAR3DHandle, &(gARHandle->markerInfo[k]), gPatt_width, gPatt_trans);
			gPatt_found = TRUE;
		} else {
			gPatt_found = FALSE;
		}

		// Tell GLUT the display has changed.
		glutPostRedisplay();
	}
}

static void cleanup(void)
{
	arglCleanup(gArglSettings);
    gArglSettings = NULL;
	arPattDetach(gARHandle);
	arPattDeleteHandle(gARPattHandle);
	arVideoCapStop();
	ar3DDeleteHandle(&gAR3DHandle);
	arDeleteHandle(gARHandle);
    arParamLTFree(&gCparamLT);
	arVideoClose();
}

//
//	This function is called on events when the visibility of the
//	GLUT window changes (including when it first becomes visible).
//
static void Visibility(int visible)
{
    clog << "Visibility -----" << endl;
	if (visible == GLUT_VISIBLE) {
		glutIdleFunc(mainLoop);
	} else {
		glutIdleFunc(NULL);
	}
}

//
//	This function is called when the
//	GLUT window is resized.
//
static void Reshape(int w, int h)
{
    clog << "Reshape -----" << endl;

    windowWidth = w;
    windowHeight = h;

	glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
	glViewport(0, 0, (GLsizei) w, (GLsizei) h);

	// Call through to anyone else who needs to know about window sizing here.
}

//
// This function is called when the window needs redrawing.
//
static void Display(void)
{
    clog << "Display -----" << endl;
    ARdouble p[16];
	ARdouble m[16];

	// Select correct buffer for this context.
	glDrawBuffer(GL_BACK);
	glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Clear the buffers for new frame.
    clog << "Display 1-----" << endl;

    arglPixelBufferDataUpload(gArglSettings, gARTImage);
    clog << "Display 2-----" << endl;
	arglDispImage(gArglSettings);
    clog << "Display 3-----" << endl;
	gARTImage = NULL; // Invalidate image data.

	// Projection transformation.
	arglCameraFrustumRH(&(gCparamLT->param), VIEW_DISTANCE_MIN, VIEW_DISTANCE_MAX, p);
	glMatrixMode(GL_PROJECTION);
#ifdef ARDOUBLE_IS_FLOAT
    glLoadMatrixf(p);
#else
    glLoadMatrixd(p);
#endif
	glMatrixMode(GL_MODELVIEW);

	glEnable(GL_DEPTH_TEST);
    clog << "Display 4-----" << endl;

	// Viewing transformation.
	glLoadIdentity();
	// Lighting and geometry that moves with the camera should go here.
	// (I.e. must be specified before viewing transformations.)
	//none

	if (gPatt_found) {

		// Calculate the camera position relative to the marker.
		// Replace VIEW_SCALEFACTOR with 1.0 to make one drawing unit equal to 1.0 ARToolKit units (usually millimeters).
		arglCameraViewRH((const ARdouble (*)[4])gPatt_trans, m, VIEW_SCALEFACTOR);
#ifdef ARDOUBLE_IS_FLOAT
        glLoadMatrixf(m);
#else
        glLoadMatrixd(m);
#endif

		// All lighting and geometry to be drawn relative to the marker goes here.
		DrawCube();

	} // gPatt_found

	// Any 2D overlays go here.
    glMatrixMode(GL_PROJECTION);
    glLoadIdentity();
    glOrtho(0, (GLdouble)windowWidth, 0, (GLdouble)windowHeight, -1.0, 1.0);
    glMatrixMode(GL_MODELVIEW);
    glLoadIdentity();
    glDisable(GL_LIGHTING);
    glDisable(GL_DEPTH_TEST);


	glutSwapBuffers();
}

static int setupCamera(const char *cparam_name, char *vconf, ARParamLT **cparamLT_p, ARHandle **arhandle, AR3DHandle **ar3dhandle)
{
    ARParam			cparam;
	int				xsize, ysize;
    AR_PIXEL_FORMAT pixFormat;

    // Open the video path.
    if (arVideoOpen(vconf) < 0) {
    	ARLOGe("setupCamera(): Unable to open connection to camera.\n");
    	return (FALSE);
	}

    // Find the size of the window.
    if (arVideoGetSize(&xsize, &ysize) < 0) {
        ARLOGe("setupCamera(): Unable to determine camera frame size.\n");
        arVideoClose();
        return (FALSE);
    }
    ARLOGi("Camera image size (x,y) = (%d,%d)\n", xsize, ysize);

	// Get the format in which the camera is returning pixels.
	pixFormat = arVideoGetPixelFormat();
	if (pixFormat == AR_PIXEL_FORMAT_INVALID) {
    	ARLOGe("setupCamera(): Camera is using unsupported pixel format.\n");
        arVideoClose();
		return (FALSE);
	}

	// Load the camera parameters, resize for the window and init.
    if (arParamLoad(cparam_name, 1, &cparam) < 0) {
		ARLOGe("setupCamera(): Error loading parameter file %s for camera.\n", cparam_name);
        arVideoClose();
        return (FALSE);
    }
    if (cparam.xsize != xsize || cparam.ysize != ysize) {
        ARLOGw("*** Camera Parameter resized from %d, %d. ***\n", cparam.xsize, cparam.ysize);
        arParamChangeSize(&cparam, xsize, ysize, &cparam);
    }
#ifdef DEBUG
    ARLOG("*** Camera Parameter ***\n");
    arParamDisp(&cparam);
#endif
    if ((*cparamLT_p = arParamLTCreate(&cparam, AR_PARAM_LT_DEFAULT_OFFSET)) == NULL) {
        ARLOGe("setupCamera(): Error: arParamLTCreate.\n");
        return (FALSE);
    }

    if ((*arhandle = arCreateHandle(*cparamLT_p)) == NULL) {
        ARLOGe("setupCamera(): Error: arCreateHandle.\n");
        return (FALSE);
    }
    if (arSetPixelFormat(*arhandle, pixFormat) < 0) {
        ARLOGe("setupCamera(): Error: arSetPixelFormat.\n");
        return (FALSE);
    }
	if (arSetDebugMode(*arhandle, AR_DEBUG_DISABLE) < 0) {
        ARLOGe("setupCamera(): Error: arSetDebugMode.\n");
        return (FALSE);
    }
	if ((*ar3dhandle = ar3DCreateHandle(&cparam)) == NULL) {
        ARLOGe("setupCamera(): Error: ar3DCreateHandle.\n");
        return (FALSE);
    }

	if (arVideoCapStart() != 0) {
    	ARLOGe("setupCamera(): Unable to begin camera data capture.\n");
		return (FALSE);
	}

	return (TRUE);
}

int main(int argc, char **argv)
{
    std::string originalWindowName = "Original";
    std::string resultWindowName = "Result";
    bool fileMode = false;
    if(std::string(argv[3]) == "FILE")
    {
        if(argc != 6)
        {
            std::cerr << "Format : vocab calibration FILE file maskfile\n";
            return 0;
        }
        fileMode = true;
    }
    else if (std::string(argv[3]) == "CAMERA")
    {
        fileMode = false;
    }
    else
    {
        std::cerr << "Format : vocab calibration FILE/CAMERA [file] [maskfile]\n";
        return 0;
    }

    cv::VideoCapture webcam;
    cv::VideoCapture videoFile;
    cv::VideoCapture maskFile;
    if(fileMode == false)
    {
        webcam.open(0);

        if( !webcam.isOpened() )
        {
            std::cerr << "***Could not initialize capturing...***\n";
            std::cerr << "Current parameter's value: \n";
            return -1;
        }
    }
    else
    {
        videoFile.open(argv[4]);
        maskFile.open(argv[5]);
    }

    #ifdef SIMPLIFY

    #else
      // Create SLAM system. It initializes all system threads and gets ready to process frames.
      ORB_SLAM2::System SLAM(argv[1],argv[2],ORB_SLAM2::System::MONOCULAR,true); //vocab yaml mono true
    #endif

    // Main loop -----------------------------------------------------------
    cv::Mat im;
    cv::Mat cameraPose;
    cv::Mat mask;
    cv::Mat output;
    cv::Mat plane = cv::Mat::zeros(4000,4000, CV_8UC3);
    int temp = 0;
    float planePos[4][4] = { {1 , 0 , 0 , 0 },
                       {0 , 1 , 0 , 0 },
                       {0 , 0 ,1 , -1},
                       {0 , 0 , 0 , 1 } };

    cv::Mat planePosition = cv::Mat(4,4,CV_32FC1, planePos);
    cout << "PLANEPOS = "<< endl << " "  << planePosition << endl << endl;

    while(1)
    {
        if(fileMode)
        {
            //argv[4] should be real video, and argv[5] should be the human mask video
            videoFile>>im;
            maskFile>>mask;

            if(!im.data || !mask.data)
            {
                break;
            }
        }
        else
        {
            // im is from live camera
            webcam >> im;
            //TODO: get the mask out of im
            //TODO: how to run python from here?
        }

/*
#ifdef COMPILEDWITHC11
        std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
#else
        std::chrono::monotonic_clock::time_point t1 = std::chrono::monotonic_clock::now();
#endif
*/

        #ifdef SIMPLIFY

        char glutGamemode[32];
        char cparam_name[] = "camera_para.dat";
        //char vconf[] = "filesrc location=./planar.avi ! decodebin ! ffmpegcolorspace ! capsfilter caps=video/x-raw-rgb,bpp=24 ! identity name=artoolkit ! fakesink";
        char vconf[] = "";
        //char vconf[] = "filesrc location=planar.avi ! decodebin ! ffmpegcolorspace ! capsfilter caps=video/x-raw-rgb,bpp=24 ! identity name=artoolkit ! fakesink";
        char patt_name[]  = "hiro.patt";

        clog << "pkosfpokadopk = " << endl;

        glutInit(&argc, argv);

    	if (!setupCamera(cparam_name, vconf, &gCparamLT, &gARHandle, &gAR3DHandle)) {
    		ARLOGe("main(): Unable to set up AR camera.\n");
    		exit(-1);
    	}

        glutInitWindowSize(windowWidth, windowHeight);
        glutCreateWindow(argv[0]);

        clog << "addpojawdojdwajd = " << endl;


        /*
        // Setup ARgsub_lite library for current OpenGL context.
        if ((gArglSettings = arglSetupForCurrentContext(&(gCparamLT->param), arVideoGetPixelFormat())) == NULL) {
            ARLOGe("main(): arglSetupForCurrentContext() returned error.\n");
            cleanup();
            exit(-1);
        }
        */

        clog << "poijsfpojifadpjiofad = " << endl;
        //arglSetupDebugMode(gArglSettings, gARHandle);
    	arUtilTimerReset();
        clog << "efoifpp@qp@wpwpw = " << endl;

    	// Load marker(s).
    	if (!setupMarker(patt_name, &gPatt_id, gARHandle, &gARPattHandle)) {
    		ARLOGe("main(): Unable to set up AR marker.\n");
    		cleanup();
    		exit(-1);
    	}
        clog << "iajfia@qp@wpwpw = " << endl;

        // Register GLUT event-handling callbacks.
        // NB: mainLoop() is registered by Visibility.
        glutDisplayFunc(Display);
        glutReshapeFunc(Reshape);
        glutVisibilityFunc(Visibility);
        //glutKeyboardFunc(Keyboard);

        glutMainLoop();

          // From ARToolKit
          //cameraPose =
        #else
          // Pass the image to the SLAM system
          SLAM.TrackMonocular(im,0); //lets look later how the system uses that "timestamp"
          // this function create new Mat which I did not freed.
          // in the long run this might cause big memory leaks?
          cameraPose = SLAM.GetCurrentCameraMatrix();
        #endif

        //string ty =  type2str( cameraPose.type() );
        //printf("Matrix: %s %dx%d \n", ty.c_str(), cameraPose.cols, cameraPose.rows );

        // This is a log to view camera pose. Used with watch command on the file.
        std::filebuf logbuf ;
        logbuf.open( "cameraPose.txt", std::ios::out|std::ios::trunc) ;
        std::streambuf* old_clog_buf = std::clog.rdbuf( &logbuf ) ;

        clog << "CameraPose = "<< endl << " "  << cameraPose << endl << endl;

        //double ttrack= std::chrono::duration_cast<std::chrono::duration<double> >(t2 - t1).count();

        // Temp info I will hardcode : bg is a plane, and human is fixed in place. Distace to a plane is known. Camera only move sideways.
        // Question : What is the unit of stored camera position data?

        output = im.clone();

        // Iterate through the mask of this frame and find alternative color
        if(!cameraPose.empty())
        {
            for(int i=0; i < mask.rows; i++) //y
            {
                for(int j=0; j < mask.cols; j++) //x
                {
                    //store data in imaginary plane
                    //do ortho projection for every point
                    float pointPos[4][4] = {   {1 , 0 , 0 , (j-320)/2000.0},
                        {0 , 1 , 0 , (i-240)/2000.0},
                        {0 , 0 , 1 , 0 },
                        {0 , 0 , 0 , 1 } };

                    cv::Mat pointPosition = cv::Mat(4,4,CV_32FC1, pointPos);
                    cv::Mat pointInWorld = pointPosition * cameraPose;
                    cv::Mat pointFromPlane = planePosition * pointInWorld;

                    //cout << "1";

                    float rotationA = pointFromPlane.at<float>(0,2);
                    float rotationB = pointFromPlane.at<float>(1,2);
                    float rotationC = pointFromPlane.at<float>(2,2);

                    //find value that makes z 0 on the line
                    // zPoint + tC = 0 -> what is t?
                    // t = -zPoint/C
                    float tValue = -pointFromPlane.at<float>(2,3) / rotationC;

                    // Use t value to get x and y
                    int xPoint = ( (pointFromPlane.at<float>(0,3)+(tValue*rotationA))*2000 ) + 2000;
                    int yPoint = ( (pointFromPlane.at<float>(1,3)+(tValue*rotationB))*2000 ) + 2000;


                    //clog << "PointPosition = "<< endl << " "  << pointPosition << endl << endl;
                    //clog << "PointFromPlane = "<< endl << " "  << pointFromPlane << endl << endl;

                    //int xPoint = round(pointFromPlane.at<float>(0,3)*5000) + 1000;
                    //int yPoint = round(pointFromPlane.at<float>(1,3)*5000) + 1000;


                    /*

                       cout << "Point = "<< " "  << xPoint << " " << yPoint << "i j = " << i << " " << j << endl;
                       if( i > temp)
                       {
                       temp = i;
                       }
                       cout << "MAX " << temp << endl;
                     */

                    if(mask.at<cv::Vec3b>(i,j)[0] < 128)
                    {
                        plane.at<cv::Vec3b>(xPoint,yPoint)[0] = (im.at<cv::Vec3b>(i,j)[0] + plane.at<cv::Vec3b>(xPoint,yPoint)[0])/2.0;
                        plane.at<cv::Vec3b>(xPoint,yPoint)[1] = (im.at<cv::Vec3b>(i,j)[1] + plane.at<cv::Vec3b>(xPoint,yPoint)[1])/2.0;
                        plane.at<cv::Vec3b>(xPoint,yPoint)[2] = (im.at<cv::Vec3b>(i,j)[2] + plane.at<cv::Vec3b>(xPoint,yPoint)[2])/2.0;

                        output.at<cv::Vec3b>(i,j)[0] = im.at<cv::Vec3b>(i,j)[0];
                        output.at<cv::Vec3b>(i,j)[1] = im.at<cv::Vec3b>(i,j)[1];
                        output.at<cv::Vec3b>(i,j)[2] = im.at<cv::Vec3b>(i,j)[2];
                    }
                    else
                    {
                        //find replacement from plane
                        output.at<cv::Vec3b>(i,j)[0] = plane.at<cv::Vec3b>(xPoint,yPoint)[0];
                        output.at<cv::Vec3b>(i,j)[1] = plane.at<cv::Vec3b>(xPoint,yPoint)[1];
                        output.at<cv::Vec3b>(i,j)[2] = plane.at<cv::Vec3b>(xPoint,yPoint)[2];
                    }
                    //cout << "2";
                }
            }
        }

        //TODO : Store camera pose and position. Store all the colors of that pose into plane. Masked pixels does not count.

        //TODO : For current image, attempt to replace masked pixels with something else. Try to interpolate colors from the plane if found color data.
        //cout << "3333";

        cv::Mat resized;
        cv::resize(plane,resized,cv::Size(500,500));


        //displaying result
        #ifdef SIMPLIFY
        #else
          SLAM.FeedAuxDisplay(resized);
          SLAM.FeedOutputDisplay(output);
        #endif

        usleep((1/(float)20)*1e6); //the wait

        std::clog << std::flush ;
        std::clog.rdbuf( old_clog_buf ) ;
    }

    #ifdef SIMPLIFY
    #else
    // Stop all threads
    SLAM.Shutdown();
    #endif

/*
    // Tracking time statistics
    sort(vTimesTrack.begin(),vTimesTrack.end());
    float totaltime = 0;
    for(int ni=0; ni<nImages; ni++)
    {
        totaltime+=vTimesTrack[ni];
    }
    cout << "-------" << endl << endl;
    cout << "median tracking time: " << vTimesTrack[nImages/2] << endl;
    cout << "mean tracking time: " << totaltime/nImages << endl;

    // Save camera trajectory
    SLAM.SaveKeyFrameTrajectoryTUM("KeyFrameTrajectory.txt");
*/

    cout << "End of program!" << endl << endl;
    while(1)
    {
    }
    return 0;
}
